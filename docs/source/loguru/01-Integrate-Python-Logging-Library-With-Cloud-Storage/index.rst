Integrate Python Logging Library With Cloud Storage
==============================================================================
Python 社区有很多 logging 的库. 但是大部分的库都是将 Log 打到 stdout 或 stderr. 在生产应用中, 我们通常要将这些日志发送到云存储中或是 Stream 中. 这该怎么做呢?

大部分的 logging 库都支持自定义 Handler. Handler 本质上是一个接受将 log message 作为输入的一个函数. 这个函数可以做任何事情. 我们很容易就能想到在这个函数中用 API 将 log 发到 Cloud 上. 这开了个好头, 下面我们来看看这个函数应该怎么设计.

Log 数据的本质是时间序列数据, 也可以理解为一个 Stream. 通常生产 Stream 数据的客户端程序都需要有下面几个刚需功能:

1. 把多个 log 打包成一个 batch 用一个 API 发出去, 这样有助于充分利用网络带宽, 避免了每个 API 都需要建立 HTTP 的开销.
2. 失败重试. 云端的系统在 log 客户端看来是外部系统, 是不稳定的. 我们必须要有个失败重试机制, 如果无法将数据发出去, 就需要等待重试. 但这又引入了新问题, 等待重试不能阻塞正常的程序运行, 我们需要将数据缓存在本地, 等到云端系统恢复了再重试. 但这又引入了新问题, 本地内存缓存是有限的, 我们必须要将其落盘保存.
3. 数据落盘缓存. 因为 batch 的机制, log 客户端如果在 buffer 的过程中挂掉了, 那么 buffer 中的数据就丢失了, 这是不能接受的. 我们通常要有跟数据库中 Write ahead log (WAL) 类似的机制将数据先落盘. 如果客户端挂掉还能从磁盘中恢复数据. 这个磁盘系统也得是容错的, 例如 AWS EFS 系统. 因为万一 client 的运行环境彻底挂了, 但数据还在磁盘上.

由此可见, 我们很难说将这么多功能放在这一个函数中. 所以这个函数本身其实没有那么重要, 重要的是有一个 Producer 的类, 能够实现这些功能并提供一套简单的 API, 让用户只需要 produce message 即可. 并且还要提供一些例如 Decorator 之类的语法糖, 可以自动包装一个上下文函数.